{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f76b0c9c-ff7b-49fa-8173-e790ca4b2be0",
   "metadata": {},
   "source": [
    "# Description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef35016-bf8f-41ba-9e64-dabf9d7e106e",
   "metadata": {},
   "source": [
    "[describe]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b54c357a-2ed8-4344-8f96-05dd2cc6c1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "import utilities as util\n",
    "\n",
    "# ignore depreciation warnings in this code\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# define the locations to save the figures into\n",
    "fig_main_savepath='./paper_figures/main/'\n",
    "fig_si_savepath='./paper_figures/si/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66bab267-ed19-4cc2-896d-22151daeb02c",
   "metadata": {},
   "source": [
    "# Load & Organize Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "308be93e-bcb5-4296-9ede-399a29de9512",
   "metadata": {},
   "source": [
    "### Berkeley (land/metric monthly data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3023bf48-8a6d-4599-80ac-481b37ef474c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the berkeley daily maximum temperature data pathway\n",
    "berkdpath='./data/berkeley/Berkeley_TMAX_land_188001_201712_daily_N96_365days_degC.nc'\n",
    "# load in the daily maximum temperature rasters \n",
    "bddat=xr.open_dataset(berkdpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a79b7f5-18e9-488a-9d94-524f05f793aa",
   "metadata": {},
   "source": [
    "#### Organize time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1c68d4e-fc2f-4ecb-bbfa-ff3bc8bf2ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the datetime grid for Berkeley monthly and daily data\n",
    "timegridbd=util.get_dt64(bddat)\n",
    "\n",
    "# add these back into the data structures\n",
    "bddat['time']=timegridbd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f21535-2339-4e2c-b818-239d8bb08376",
   "metadata": {},
   "source": [
    "### Get a Land-Sea Mask from the Berkeley data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "948b076c-a724-4e57-99f2-e0b8f526ac5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the lsm raster pathway\n",
    "lsmpath='./data/berkeley/berkeley_lsm.nc'\n",
    "\n",
    "# load the land-sea mask into memory\n",
    "lsm=xr.open_dataarray(lsmpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a011fb-ff4a-4aa7-bbc6-788064c8b2aa",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Replace 90S invalid data with missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a71327ca-4688-4e4a-9d3b-295274c84764",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set 90S equal to missing\n",
    "bddat=util.set_N96_SouthPole_missing(bddat,'TMAX')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e064356-fd9a-4cab-ae94-4c01314ca920",
   "metadata": {},
   "source": [
    "### GMST (global monthly timeseries, 3-year mean filtered via hadcrut_GMST.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8afea7f3-f624-4577-b330-729f84c2df65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "gmstpath='./data/analysis/GMST-3yrmean.nc'\n",
    "gmst = xr.open_dataset(gmstpath)\n",
    "# get the timeseries from 1880-2017\n",
    "gmstperiod=[1880,2017]\n",
    "gmst_slice=util.dt64_yrslice(gmstperiod[0],gmstperiod[1])\n",
    "GMST=gmst['GMST-3yrmean'].sel(time=gmst_slice)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3caf01-0c33-4b65-acca-708ea21317d0",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Find scenario Deltas from HadCRUT5 data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75515ea-7e24-401c-afc2-190b74584e58",
   "metadata": {},
   "source": [
    "The scenarios for average temperature are:\n",
    "* Counterfactual (1885-1915)\n",
    "* Climate Base Period (1985-2015)\n",
    "* IPCC Attributable Warming (2010-2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f6cd6291-9c53-4e72-99aa-9a0c9a7dbd1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counterfactual GMST (1885-1915) anomaly from CLIM (1985-2015): -0.85C\n",
      "Attributable GMST (2010-2019) anomaly from CLIM (1985-2015): 0.29C\n",
      "Attributable minus Natural: +1.13C\n"
     ]
    }
   ],
   "source": [
    "# calculate differences between periods\n",
    "natural_shift=(gmst['Counterfactual_Delta']-gmst['CLIM_Delta']).values.flatten()[0]\n",
    "forced107_shift=(gmst['Attributable_Delta']-gmst['CLIM_Delta']).values.flatten()[0]\n",
    "# and print results to screen\n",
    "print('Counterfactual GMST (1885-1915) anomaly from CLIM (1985-2015): '+str(np.round(natural_shift,2))+'C')\n",
    "print('Attributable GMST (2010-2019) anomaly from CLIM (1985-2015): '+str(np.round(forced107_shift,2))+'C')\n",
    "diff_ATT_minus_NAT=forced107_shift-natural_shift\n",
    "print('Attributable minus Natural: +'+str(np.round(diff_ATT_minus_NAT,2))+'C')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83dc5799-52d0-45fd-bb9e-949b5ad866e6",
   "metadata": {},
   "source": [
    "### Calculate and Store Monthly medians"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094ec624-ec39-4014-a1ed-c8b4f31a376d",
   "metadata": {},
   "source": [
    "Now at every spatial location let's calculate the 50th percentile in each month from the daily data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f3d9e18c-9517-404c-a8a3-e7735c61b174",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the median quantile we are calculating over\n",
    "qi=0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b8e32deb-1f8d-4e93-833f-c280d02eb3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab each month's data,\n",
    "# e.g. month_idxs[1] IS JANUARY (idx=0:30,365:395,etc.), and so on for each month\n",
    "month_idxs=bddat.groupby('time.month').groups\n",
    "monthsi=np.arange(1,13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21762fe5-f83e-424d-9a4d-a181093131df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mon index=1\n"
     ]
    }
   ],
   "source": [
    "# loop over each of the months in the data set\n",
    "for mi in monthsi:\n",
    "    \n",
    "    # find the median timeseries for a fixed month across the days in that month, for each year\n",
    "    moni=bddat.isel(time=month_idxs[mi])\n",
    "    store=moni.groupby('time.year').quantile(qi)\n",
    "    del moni\n",
    "    \n",
    "    # create and assign time index, to assist merging, using central day/time of month as index\n",
    "    tgrid=bddat['time'].isel(time=month_idxs[mi]).groupby('time.year').map(util.middle_element)\n",
    "    store2=(\n",
    "        store\n",
    "            .assign_coords({'year': tgrid})\n",
    "            .drop('time')\n",
    "        )\n",
    "    del store\n",
    "    \n",
    "    # merge into a single dataset\n",
    "    if mi==1:\n",
    "        q50i=store2['TMAX']\n",
    "    else:\n",
    "        q50i=xr.merge([q50i,store2])\n",
    "    del store2\n",
    "    # where are we in the loop?\n",
    "    print('mon index='+str(mi))\n",
    "\n",
    "# rename the time dimension\n",
    "q50=(\n",
    "    q50i\n",
    "        .rename_dims({\"year\":\"time\"})      \n",
    "        .rename({\"year\":\"time\"})   \n",
    "    )\n",
    "# q50['TMAX'].attrs\n",
    "del q50i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5faf56a3-519a-400a-a4f3-cb651e4bf016",
   "metadata": {},
   "outputs": [],
   "source": [
    "# q50['TMAX'].attrs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f1de7e-8c26-42c4-be4a-8699678ec902",
   "metadata": {},
   "source": [
    "### Save out q50 array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a249f581-4993-4822-86bd-2aafceeebfbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save out to a netcdf file now that we have the Q50 data array\n",
    "q50savepath='./data/berkeley/q50_monthly_1880-2017.nc'\n",
    "q50.to_netcdf(q50savepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf1c98f-e291-4368-9e41-aece736a6e50",
   "metadata": {},
   "source": [
    "# Calculate Median Scale Factors ($\\beta$): Regress q50 and GMST"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db2f3b1-00dc-484c-bc53-cec5eca1fcd1",
   "metadata": {},
   "source": [
    "We regress median maximum temperatures at every location (and for each month) against GMST, ignoring missing years in the regression. The resulting slope is the scale factor, $\\beta$, used to scale the climatological distribution, to arrive at the forced and counterfactual distributions used in the observation-based attribution analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44391be-756f-4a21-ba88-f7c703f80531",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AttributionKernel",
   "language": "python",
   "name": "att"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
